{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93a80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9548de7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb84427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using modin for faster dataframe processing\n",
    "# import ray\n",
    "# ray.init()\n",
    "# import modin.pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import re\n",
    "from dateutil import parser as date_parser\n",
    "from datetime import datetime\n",
    "import geocoder\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "# spacy.cli.download('en_core_web_lg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# config contains config required for bigquery table\n",
    "from config import table_id\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f781257",
   "metadata": {},
   "source": [
    "Read data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9792eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"ufoReports.json\", orient=\"records\", dtype=object, convert_dates=False)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0004f8ef",
   "metadata": {},
   "source": [
    "Dropping redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e1353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['location', 'LOCATION', 'OCCURRED', 'REPORTED', 'DURATION', 'SHAPE'], axis=1)\n",
    "# Not going to use this data\n",
    "df = df.drop(columns=['Posted'], axis=1)\n",
    "df = df.rename(columns={\"Characteristics\": \"Report\"})\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7832e5",
   "metadata": {},
   "source": [
    "Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f66a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_index = df[df.duplicated([\"Occurred\", \"Location\", \"Report\"])].index\n",
    "print(f'number of rows={len(df)}, duplicated rows={len(duplicate_index)}')\n",
    "df = df.drop(duplicate_index)\n",
    "print(f'number of rows after deletion: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dfba95",
   "metadata": {},
   "source": [
    "Covert Occurred and reported date time to proper format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbdd136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used \"(O|R|P).*\":\\s\"\\s*\\d{4} to check date formate in data\n",
    "def convert_date_time(text):\n",
    "    try:\n",
    "        # pattern ='date' 'time' 'AM' or 'PM' if it is mentioned\n",
    "        date_time_pattern = re.compile(r'\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{1,2}(:\\d{1,2})?\\s?(AM|PM)?', re.IGNORECASE)\n",
    "        # timeZone = tz.gettz(TimezoneFinder().timezone_at(latlang))\n",
    "        string_time = date_time_pattern.search(text).group()\n",
    "        return date_parser.parse(string_time) if string_time else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "df['TimeOfEvent'] = df.Occurred.apply(convert_date_time, convert_dtype=False)\n",
    "df['ReportedTime'] = df.Reported.apply(convert_date_time, convert_dtype=False)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67890a3d",
   "metadata": {},
   "source": [
    "Transform Duration Strings to seconds(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_seconds(string):\n",
    "    try:\n",
    "        # Following commented method fail to parse strings reported in different formate,\n",
    "        # gosh! have to do string manipulation again\n",
    "        # dt = date.today()\n",
    "        # #make dt as midnight time\n",
    "        # dt = datetime.combine(dt, datetime.min.time())\n",
    "        # #time from string dn calculated as today's date + time\n",
    "        # dn = date_parser.parse(string)\n",
    "        # time_string = str(dn - dt)\n",
    "        # h, m, s = map(int, time_string.split(':'))\n",
    "        # return int(h * 3600 + m * 60 + s)\n",
    "\n",
    "        # extract total seconds using string manipulation\n",
    "        # TODO: There are some cases which can be parsed with use of word2number package for e.g \"one-two minutes\"\n",
    "        # TODO: Unable to parse cases like 2-minutes, figure out different cases for better results\n",
    "        h_patrn = re.compile(r'(\\d{1,2})\\s?(hr|hour)', re.IGNORECASE)\n",
    "        m_patrn = re.compile(r'(\\d{1,2})\\s?(min|minute)', re.IGNORECASE)\n",
    "        s_patrn = re.compile(r'(\\d{1,2})\\s?(sec|second)', re.IGNORECASE)\n",
    "        try:\n",
    "            h = int(h_patrn.search(string).group(1))\n",
    "        except:\n",
    "            h = 0\n",
    "        try:\n",
    "            m = int(m_patrn.search(string).group(1))\n",
    "        except:\n",
    "            m = 0\n",
    "        try:\n",
    "            s = int(s_patrn.search(string).group(1))\n",
    "        except:\n",
    "            s = 0\n",
    "        total_sec = h * 3600 + m * 60 + s\n",
    "        # when fail to parse\n",
    "        if total_sec == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return total_sec\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "df['DurationInSecond'] = df.Duration.apply(calculate_seconds, convert_dtype=False)\n",
    "df['DurationInSecond'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdee4d7",
   "metadata": {},
   "source": [
    "Clean 'Shape' column. Transform column to have categorical data(one word representing shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc15945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing shape value as Unknown and use Unknown for Other type\n",
    "def fill_shape(x):\n",
    "    x = x.strip().lower()\n",
    "    if x:\n",
    "        if x == \"other\":\n",
    "            return \"unknown\"\n",
    "        else:\n",
    "            return x\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "df['Shape'] = df['Shape'].apply(fill_shape)\n",
    "\n",
    "# Check if shape column needs further cleaning\n",
    "shape_uniq = df['Shape'].unique()\n",
    "print(f\"Unique shapes:\\n{shape_uniq}\")\n",
    "\n",
    "# make shape category if shape has one word to describe it\n",
    "cat_filter = []\n",
    "for shape in shape_uniq:\n",
    "    # no space = one word since the shapes were stripped earlier\n",
    "    cat_filter.append(not shape.__contains__(\" \"))\n",
    "shape_categories = shape_uniq[cat_filter]\n",
    "print(f\"\\nShape categories:\\n{shape_categories}\")\n",
    "\n",
    "# Pattern to identify shape from given description\n",
    "shapes_cat_pattern = re.compile(r'\\b(' + r'|'.join(shape_categories) + r')\\b')\n",
    "\n",
    "\n",
    "# try to find shape when sentences are passed instead of one word\n",
    "def categorize_shape(shape_str):\n",
    "    # since all shape categories are of one word\n",
    "    if shape_str.__contains__(\" \"):\n",
    "        try:\n",
    "            # Problematic in cases like: sentence is something like \"light coming out of triangle\"\n",
    "            #  then light will be used instead triangle\n",
    "            #  OR if word triangular is used instead triangle\n",
    "            shape = shapes_cat_pattern.search(shape_str).group(1)\n",
    "            return shape\n",
    "        except:\n",
    "            return \"unknown\"\n",
    "    else:\n",
    "        return shape_str\n",
    "\n",
    "\n",
    "# Clean Shape\n",
    "df['Shape'] = df['Shape'].apply(categorize_shape)\n",
    "print(df['Shape'].value_counts())\n",
    "df['Shape'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880d4cf",
   "metadata": {},
   "source": [
    "Remove rows which couldn't be transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd8b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbt = df.shape\n",
    "# Dropping rows with missing value\n",
    "df = df.dropna(subset=[\"TimeOfEvent\", \"ReportedTime\", \"Shape\"], axis=0, how=\"any\")\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "df.drop(columns=[\"Occurred\", \"Reported\", \"Duration\"], axis=1)\n",
    "rat = df.shape\n",
    "print(df.info())\n",
    "print(f\"\\n##Dropping some missing values\\n\"\n",
    "      f\"rows dropped={rbt[0] - rat[0]}, Before:{rbt}, after:{rat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab6789",
   "metadata": {},
   "source": [
    "Calculate Day of the event occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439087d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DayOfEvent'] = df['TimeOfEvent'].apply(lambda x: x.strftime(\"%A\"))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24a02c",
   "metadata": {},
   "source": [
    "Clean 'DurationInSecond' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function so missing values can be treated as NaN\n",
    "df['DurationInSecond'] = df['DurationInSecond'].apply(pd.to_numeric)\n",
    "\n",
    "# looking for outliers\n",
    "sns.boxplot(x=df['DurationInSecond'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d230671",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plt for duration to check for outliers\n",
    "sns.scatterplot(x=df['TimeOfEvent'], y=df['DurationInSecond'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c197b2",
   "metadata": {},
   "source": [
    "Will Use Z score for outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = df.shape\n",
    "\n",
    "# calculate z score to find outliers in DurationInSecond\n",
    "z = np.abs(stats.zscore(df[\"DurationInSecond\"], nan_policy='omit'))\n",
    "# as common practice, remove z where abs(z) > 3\n",
    "delete_indexes = z[z > 3].index\n",
    "df = df.drop(delete_indexes, axis=0)\n",
    "sa = df.shape\n",
    "print(f\"\\n\\nDeleted Outliers where absolute z score of DurationInSecond> 3\\n\"\n",
    "      f\"rows deleted={sb[0] - sa[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43dafa2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3219a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curious to see if shapes correlate to Duration UFO were visible\n",
    "sns.scatterplot(x=df['Shape'], y=df['DurationInSecond'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc478957",
   "metadata": {},
   "source": [
    "Fill missing Duration in second value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0fce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null duration with mean\n",
    "# would replace with closest neighbour would have been better instead?\n",
    "meanDuration = df['DurationInSecond'].mean()\n",
    "df['DurationInSecond'] = df['DurationInSecond'].fillna(meanDuration)\n",
    "\n",
    "# Dropping original Duration column since we have transformed it to values in seconds\n",
    "del (df['Duration'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec86478",
   "metadata": {},
   "source": [
    "Some stuff to explore Corpus, and to determine things to consider before finding TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0104002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal is to give weight to reports based on uniqueness of the report. (We want more Information gain haha)\n",
    "# Summation of IDF values of tokens in a document can be used to assign the weight\n",
    "# but using TFIDF can leverage unique repeating words within document.\n",
    "# However, TFIDF will be more prone to number of tokens in a document compared to only IDF\n",
    "# since using TF is dependent of doc's length.\n",
    "# More experiments should be conducted to determine better method to assign weight to the report.\n",
    "\n",
    "\n",
    "# stopword pattern\n",
    "stopword_pattern = re.compile(r'(\\b(' + r'|'.join(stopwords) + r')(\\b))|(\\\\+n)')\n",
    "# resetting index to make sure we have unique indexes\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Remove stop words and tokenize report into words with lowercase\n",
    "def get_useful_tokens(report):\n",
    "    if not isinstance(report, str):\n",
    "        return []\n",
    "    report = report.lower()\n",
    "    report = stopword_pattern.sub('', report)\n",
    "    # only using words that starts with alphabets. When tokenizing based on \\w only, it extracted  many unnecessary\n",
    "    # tokens\n",
    "    token_list = re.findall(r'(\\b[a-z]\\w+\\b)', report)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "# Find corpus specific stopwords\n",
    "def find_c_specific_stopwords():\n",
    "    # Tokens in corpus with general stopwords removed\n",
    "    c_tokens = df['Report'].apply(get_useful_tokens)\n",
    "\n",
    "    # iterate over rows and then withing that iteration iterate over tokens to flatten it\n",
    "    # indexes are same for tokens in a row\n",
    "    f_tokens = pd.DataFrame(\n",
    "        [(index, value) for (index, values) in c_tokens.iteritems()\n",
    "         for value in values],\n",
    "        columns=['index', 'tokens']\n",
    "    ).set_index('index')\n",
    "    token_count = f_tokens.value_counts()\n",
    "\n",
    "    # making a representable dataframe\n",
    "    token_count = token_count.reset_index()\n",
    "    token_count = token_count.rename(columns={0: \"Count\"})\n",
    "    print(\"Tokens and their counts:\\n\", token_count)\n",
    "\n",
    "    # So there are many unnecessary tokens that starts with numbers and there are tokens that are misspelled. Not\n",
    "    # dealing with misspelled tokens right now because it is time consuming with Spacy's contextualSpellCheck module.\n",
    "    # If we want to correct spellings, have to find a better way.\n",
    "    # token_count.to_csv('tokenCounts.csv')\n",
    "\n",
    "    # using boxplot to check what counts are too many (outliers)\n",
    "    sns.boxplot(x=token_count['Count'])\n",
    "    plt.title('What counts are too much compared to others')\n",
    "    plt.show()\n",
    "\n",
    "    # if count > 45k calling it a corpus specific stopword\n",
    "    cond = token_count['Count'] > 45000\n",
    "    c_stopwords = token_count['tokens'][cond].to_list()\n",
    "    print(\"Corpus specific stopwords: \", c_stopwords)\n",
    "    stopwords.update(c_stopwords)\n",
    "    return c_stopwords\n",
    "\n",
    "\n",
    "# Update stopwords with corpus specific stop words\n",
    "stopwords.update(find_c_specific_stopwords())\n",
    "stopword_pattern = re.compile(r'(\\b(' + r'|'.join(stopwords) + r')(\\b))|(\\\\+n)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0094f4e",
   "metadata": {},
   "source": [
    "Perform a rule based lemmatization on a given Report (Description by user) then remove stopwords and tokenize words to find TF(term frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "It takes about an hour when using single core. have tried multiple methods but spacy's pipline for lemmatization is our bottleneck\n",
    "Tried passing docs to cython code for faster iteration over tokens\n",
    "Tried passing giant text (concatenation of df['Report'] using pandas.series.str.cat)\n",
    "into pipeline but there is no improvement in timing.\n",
    "Tried chunking the giant text but the there is no significant improvement since nlp() or spacy is our bottleneck.\n",
    "There is no significant improvement when pipeline trained over small dataset(en_core_web_sm) is used\n",
    "so using en_core_web_lg.\n",
    "'''\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\", exclude=['parser', 'ner'])\n",
    "nlp.enable_pipe('senter')\n",
    "null_doc_count = 0\n",
    "\n",
    "\n",
    "# Lemmatize Report, convert text to lowercase, and remove stop words\n",
    "def text_preprocess(report):\n",
    "    if not isinstance(report, str):\n",
    "        global null_doc_count\n",
    "        null_doc_count = null_doc_count + 1\n",
    "        return \"\"\n",
    "    doc = nlp(report)\n",
    "    lemma_text = \"\"\n",
    "    for sent in doc.sents:\n",
    "        lemma_text = f'{lemma_text}{sent.lemma_}'\n",
    "    lemma_text = lemma_text.lower()\n",
    "    return stopword_pattern.sub('', lemma_text)\n",
    "\n",
    "\n",
    "st = datetime.now()\n",
    "df['Summary'] = df['Report'].apply(text_preprocess)\n",
    "print(\"Time to prep corpus\", datetime.now() - st)\n",
    "df.to_json(\"computedDF.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c46fba",
   "metadata": {},
   "source": [
    "Calculate score of report by summing up TFID values of tokens in a report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not using TFIDF from sklearn because their method seems to be more prone to number of tokens than regular one\n",
    "# Check experiment_playground for more detail\n",
    "# If we want find sum of TFID directly from scipy implementation:\n",
    "# vectorizer = TfidfVectorizer(lowercase=False, token_pattern=r'(?u)\\b[a-z]\\w+\\b')\n",
    "# tf_idf_vectors = vectorizer.fit_transform(df['Summary'].to_list())\n",
    "# return TF.sum(axis=1).A1\n",
    "\n",
    "# Count vector to count number of tokens in a report\n",
    "# consider only tokens/words that start with alphabets. It will get rid of some unnecessary tokens\n",
    "vectorizer = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b[a-z]\\w+\\b')\n",
    "TF = vectorizer.fit_transform(df['Summary'].to_list())\n",
    "\n",
    "# the resultant array will have tokens as column and their count in reports as a row\n",
    "# token_col_list = vectorizer.get_feature_names_out()\n",
    "\n",
    "# total number of documents that are not empty\n",
    "N = df['Summary'].shape[0] - null_doc_count\n",
    "\n",
    "\n",
    "# Function to calculate IDF\n",
    "def cal_idf(doc_freq):\n",
    "    return np.log(N / doc_freq)\n",
    "\n",
    "\n",
    "# number of tokens in a doc\n",
    "nt_in_doc = csr_matrix(TF.sum(axis=1).A1).transpose()\n",
    "\n",
    "# Making a IDF calculating numpy func so we can apply it to array\n",
    "idf_v = np.vectorize(cal_idf)\n",
    "# non zero entries across token column gets document frequency\n",
    "IDF = csr_matrix(idf_v(TF.getnnz(axis=0))).transpose()\n",
    "\n",
    "# multiply count vector matrix to matrix of token's\n",
    "# IDF (matrix of one column, rows=IDF of tokens which are in same sequence as token columns in countvector matrix)\n",
    "TFIDF_SUM = TF._mul_sparse_matrix(IDF)\n",
    "\n",
    "# Dividing by number of tokens to use general formula of TF\n",
    "# do division where condition is met otherwise return 0. This will avoid division by 0 problem\n",
    "score = pd.Series(TFIDF_SUM._divide_sparse(nt_in_doc).A1)\n",
    "score = score.fillna(0)\n",
    "df['Report_score'] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f86e2",
   "metadata": {},
   "source": [
    "Geocode Location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes around 30 hrs because arcgis api limits 1 api call per second (brutal for our 133k+ records).\n",
    "# TODO: There might be better approach like api with no limit or with any offline database\n",
    "# using latitude longitude so it is easy to put on a map and find timeZone if needed\n",
    "def getlatlang(loc):\n",
    "    try:\n",
    "        location = geocoder.arcgis(loc)\n",
    "        return location.latlng\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "df[\"LatLng\"] = df.Location.apply(lambda x: getlatlang(x) if x else None)\n",
    "df = df.dropna(subset=[\"LatLng\"], axis=0, how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e18e62",
   "metadata": {},
   "source": [
    "Dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0aa034",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Occurred', 'Reported', 'Summary'], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff1acd",
   "metadata": {},
   "source": [
    "For some reason, cant pass lat lng as array or dict or named tuple for bigquery.enums.SqlTypeNames.GEOGRAPHY type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try using pyproj or pyCRS for Open Geospatial Consortium standard\n",
    "# We can convert lat long to Geopoint which is supported by GEOGRAPHY data type of bigquery with following\n",
    "# ALTER TABLE <TableName> ADD COLUMN GeoPoint GEOGRAPHY;\n",
    "# UPDATE <TableName> SET GeoPoint=ST_GEOGPOINT(Lng, Lat) where TRUE;\n",
    "\n",
    "df['Lat'] = df['LatLng'].apply(lambda x: x[0])\n",
    "df['Lng'] = df['LatLng'].apply(lambda x: x[1])\n",
    "del (df['LatLng'])\n",
    "df.info()\n",
    "df.to_csv(\"~/Desktop/CleanUFOs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c3ab8",
   "metadata": {},
   "source": [
    "Send this to oblivion (to any external database for safe keeping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaffc6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[\n",
    "        bigquery.SchemaField('ID', bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField('TimeOfEvent', bigquery.enums.SqlTypeNames.DATETIME),\n",
    "        bigquery.SchemaField('ReportedTime', bigquery.enums.SqlTypeNames.DATETIME),\n",
    "        bigquery.SchemaField('Location', bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField('Lat', bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "        bigquery.SchemaField('Lng', bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "        bigquery.SchemaField('Shape', bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField('DurationInSecond', bigquery.enums.SqlTypeNames.FLOAT),\n",
    "        bigquery.SchemaField('Report', bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField('Report_score', bigquery.enums.SqlTypeNames.FLOAT64)\n",
    "    ],\n",
    "    # Overwrite existing data\n",
    "    write_disposition=\"WRITE_TRUNCATE\"\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(\n",
    "    df, table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "\n",
    "job.result()  # Wait for the job to complete.\n",
    "\n",
    "table = client.get_table(table_id)  # Make an API request.\n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        table.num_rows, len(table.schema), table_id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7f3a3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}